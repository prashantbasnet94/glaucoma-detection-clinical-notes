{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glaucoma Detection from Clinical Notes - Google Colab\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Enable GPU: Runtime > Change runtime type > GPU\n",
    "3. Upload `clinical_notes.csv` when prompted\n",
    "4. Run all cells\n",
    "\n",
    "**Note:** You can also clone your GitHub repo instead of uploading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Colab already has most of these)\n",
    "!pip install -q tensorflow pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "**Option 1:** Upload clinical_notes.csv manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # Click \"Choose Files\" and upload clinical_notes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:** Clone from GitHub (if you've uploaded there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify with your GitHub URL\n",
    "# !git clone https://github.com/yourusername/your-repo.git\n",
    "# %cd your-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy All Code from Your Local Files\n",
    "\n",
    "Below cells contain all the code from your project files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING CODE\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "class ClinicalNotesPreprocessor:\n",
    "    def __init__(self, max_words=10000, max_len=500):\n",
    "        self.max_words = max_words\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'\\bdate_time\\b', 'datetoken', text)\n",
    "        text = re.sub(r'\\bperson\\b', 'persontoken', text)\n",
    "        text = re.sub(r'\\blocation\\b', 'locationtoken', text)\n",
    "        text = re.sub(r'\\bphone_number\\b', 'phonetoken', text)\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'[^a-z0-9\\s\\./\\-]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        print(\"Cleaning clinical notes...\")\n",
    "        df['cleaned_note'] = df['note'].apply(self.clean_text)\n",
    "        df['label'] = (df['glaucoma'] == 'yes').astype(int)\n",
    "        df_filtered = df[df['race'].isin(['asian', 'black', 'white'])].copy()\n",
    "        print(f\"Dataset size: {len(df_filtered)}\")\n",
    "        print(f\"Glaucoma positive: {df_filtered['label'].sum()}\")\n",
    "        return df_filtered\n",
    "\n",
    "    def tokenize_and_pad(self, train_texts, val_texts, test_texts):\n",
    "        print(\"Tokenizing...\")\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(train_texts)\n",
    "        \n",
    "        train_seq = self.tokenizer.texts_to_sequences(train_texts)\n",
    "        val_seq = self.tokenizer.texts_to_sequences(val_texts)\n",
    "        test_seq = self.tokenizer.texts_to_sequences(test_texts)\n",
    "        \n",
    "        train_padded = pad_sequences(train_seq, maxlen=self.max_len, padding='post')\n",
    "        val_padded = pad_sequences(val_seq, maxlen=self.max_len, padding='post')\n",
    "        test_padded = pad_sequences(test_seq, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        print(f\"Vocab size: {len(self.tokenizer.word_index)}\")\n",
    "        return train_padded, val_padded, test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL IMPLEMENTATIONS\n",
    "\n",
    "def build_lstm_model(vocab_size, max_len):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, 128, input_length=max_len),\n",
    "        layers.SpatialDropout1D(0.2),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Bidirectional(layers.LSTM(64)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='LSTM')\n",
    "    return model\n",
    "\n",
    "def build_gru_model(vocab_size, max_len):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, 128, input_length=max_len),\n",
    "        layers.SpatialDropout1D(0.2),\n",
    "        layers.Bidirectional(layers.GRU(128, return_sequences=True)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Bidirectional(layers.GRU(64)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='GRU')\n",
    "    return model\n",
    "\n",
    "def build_cnn_model(vocab_size, max_len):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, 128, input_length=max_len),\n",
    "        layers.SpatialDropout1D(0.2),\n",
    "        layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv1D(128, 5, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='CNN')\n",
    "    return model\n",
    "\n",
    "def build_transformer_model(vocab_size, max_len):\n",
    "    inputs = layers.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(vocab_size, 128)(inputs)\n",
    "    \n",
    "    positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "    pos_emb = layers.Embedding(max_len, 128)(positions)\n",
    "    x = x + pos_emb\n",
    "    \n",
    "    # Transformer block\n",
    "    attn = layers.MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.5)(x, x)\n",
    "    x1 = layers.Add()([x, attn])\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    \n",
    "    ffn = layers.Dense(256, activation='relu')(x1)\n",
    "    ffn = layers.Dropout(0.5)(ffn)\n",
    "    ffn = layers.Dense(128)(ffn)\n",
    "    x = layers.Add()([x1, ffn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs, name='Transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('clinical_notes.csv')\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "\n",
    "# Prepare data\n",
    "preprocessor = ClinicalNotesPreprocessor(max_words=10000, max_len=500)\n",
    "df = preprocessor.prepare_data(df)\n",
    "\n",
    "# Split data\n",
    "train_val_df = df[df['use'] == 'training']\n",
    "test_df = df[df['use'] == 'test']\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.15, \n",
    "                                     random_state=42, stratify=train_val_df['label'])\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Tokenize\n",
    "X_train, X_val, X_test = preprocessor.tokenize_and_pad(\n",
    "    train_df['cleaned_note'].values,\n",
    "    val_df['cleaned_note'].values,\n",
    "    test_df['cleaned_note'].values\n",
    ")\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "test_race = test_df['race'].values\n",
    "\n",
    "vocab_size = min(10000, len(preprocessor.tokenizer.word_index) + 1)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 30  # Reduced for Colab\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "models_dict = {\n",
    "    'LSTM': build_lstm_model(vocab_size, 500),\n",
    "    'GRU': build_gru_model(vocab_size, 500),\n",
    "    'CNN': build_cnn_model(vocab_size, 500),\n",
    "    'Transformer': build_transformer_model(vocab_size, 500)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_auc', patience=5, mode='max', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'predictions': y_pred,\n",
    "        'auc': auc\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Test AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate race-specific metrics\n",
    "print(\"\\nRace-Specific Performance:\\n\")\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    y_pred = result['predictions']\n",
    "    \n",
    "    for race in ['asian', 'black', 'white']:\n",
    "        mask = test_race == race\n",
    "        if mask.sum() > 0:\n",
    "            race_auc = roc_auc_score(y_test[mask], y_pred[mask])\n",
    "            print(f\"  {race.capitalize()}: AUC = {race_auc:.4f} (n={mask.sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    history = result['history'].history\n",
    "    \n",
    "    ax.plot(history['auc'], label='Train AUC')\n",
    "    ax.plot(history['val_auc'], label='Val AUC')\n",
    "    ax.set_title(f'{name} - Training History')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('AUC')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'AUC': [r['auc'] for r in results.values()]\n",
    "})\n",
    "\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(results_df)\n",
    "\n",
    "# Download\n",
    "files.download('model_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
